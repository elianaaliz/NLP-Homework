{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "83202f5c",
   "metadata": {},
   "source": [
    "# Parte 2 - Etapa de Pre-procesado de texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5c0cd45d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/jose/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import os\n",
    "import joblib\n",
    "\n",
    "from bs4 import BeautifulSoup \n",
    "import nltk\n",
    "nltk.download(\"stopwords\")  \n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from nltk.stem.porter import *\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.preprocessing import sequence\n",
    "import sklearn.preprocessing as pr\n",
    "from keras.layers import Embedding, LSTM, Dense, Dropout, GRUV2, SimpleRNN\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, roc_curve, precision_recall_curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b98a51a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocesado(path, size):\n",
    "    \n",
    "    '''carga de datos'''\n",
    "    df = pd.read_json(path, lines=True, \n",
    "                        compression='gzip')[:size][['reviewText', 'overall']]\n",
    "    \n",
    "    df.overall = [1 if int(row) > 2 else 0 for row in df.overall] \n",
    "    \n",
    "    '''Balanceo de etiquetas'''\n",
    "    label_1, label_0 = df['overall'].value_counts()\n",
    "\n",
    "    df = pd.concat([df[df.overall == 1].sample(label_0 * 1),\n",
    "                    df[df.overall == 0]],\n",
    "                   axis=0)    \n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        df.reviewText,\n",
    "        df.overall,   \n",
    "        test_size=0.3,\n",
    "        random_state=42,\n",
    "        shuffle=True\n",
    "    )  \n",
    "    \n",
    "    len_vocabulary = set()\n",
    "    \n",
    "    def review_to_words(review):\n",
    "        \"\"\"Convert a raw review string into a sequence of words.\"\"\"\n",
    "        text = BeautifulSoup(review, \"html5lib\").get_text()\n",
    "        text = re.sub(r\"[^a-zA-Z0-9]\", \" \", review.lower())\n",
    "        words = text.split()\n",
    "        words = [w for w in words if len(w) > 3 and w not in stopwords.words(\"english\")]\n",
    "        words = [PorterStemmer().stem(w) for w in words] \n",
    "        for w in words:\n",
    "            len_vocabulary.add(w)\n",
    "        return words\n",
    "    \n",
    "    words_train = list(map(review_to_words, list(X_train)))\n",
    "    words_test = list(map(review_to_words, X_test)) \n",
    "    \n",
    "    vectorizer = CountVectorizer(max_features = len(len_vocabulary),\n",
    "             preprocessor=lambda x: x, tokenizer=lambda x: x)  # already preprocessed   \n",
    "    \n",
    "    '''features para GradientBoostingClassifier'''\n",
    "    features_train_gradient = vectorizer.fit_transform(words_train).toarray()\n",
    "    features_test_gradient = vectorizer.transform(words_test).toarray()\n",
    "    \n",
    "    #normalizamos\n",
    "    features_train_gradient = pr.normalize(features_train_gradient, axis=1)\n",
    "    features_test_gradient = pr.normalize(features_test_gradient, axis=1)    \n",
    "    \n",
    "    '''Vocabulario'''\n",
    "    vocabulary = vectorizer.vocabulary_\n",
    "    \n",
    "    '''features para redes'''    \n",
    "    def features(words):\n",
    "        features = []\n",
    "        for sentence in words:            \n",
    "            words = []\n",
    "            for word in sentence:\n",
    "                try:\n",
    "                    words.append(vocabulary[word])\n",
    "                except:\n",
    "                    pass\n",
    "            features.append(words)\n",
    "        return features\n",
    "    \n",
    "    features_train = features(words_train)\n",
    "    features_test = features(words_test)      \n",
    "    \n",
    "    maxlen = max(len(max(features_train)),len(max(features_test)))\n",
    "    features_train = sequence.pad_sequences(features_train, maxlen = maxlen)    \n",
    "    features_test = sequence.pad_sequences(features_test, maxlen = maxlen)\n",
    "    \n",
    "    return X_train,\\\n",
    "           X_test,\\\n",
    "           features_train,\\\n",
    "           features_test,\\\n",
    "           np.array(y_train),\\\n",
    "           np.array(y_test),\\\n",
    "           features_train_gradient,\\\n",
    "           features_test_gradient,\\\n",
    "           list(y_train),\\\n",
    "           list(y_test),\\\n",
    "           vocabulary,\\\n",
    "           maxlen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c923a3b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!wget http://snap.stanford.edu/data/amazon/productGraph/categoryFiles/reviews_Amazon_Instant_Video_5.json.gz\n",
    "X_train,\\\n",
    "X_test,\\\n",
    "features_train,\\\n",
    "features_test,\\\n",
    "labels_train,\\\n",
    "labels_test,\\\n",
    "features_train_gradient,\\\n",
    "features_test_gradient,\\\n",
    "labels_train_gradient,\\\n",
    "labels_test_gradient,\\\n",
    "vocabulary,\\\n",
    "maxlen = preprocesado('reviews_Amazon_Instant_Video_5.json.gz',37126)     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb5ad529",
   "metadata": {},
   "source": [
    "# Parte 3 -  Etapa de entrenamiento y testeo de un modelo de análisis de sentimiento"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7073d1e7",
   "metadata": {},
   "source": [
    "## Machine Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a03b9ac9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[GradientBoostingClassifier] Accuracy: train = 0.8463521015067407, test = 0.7816836262719704\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "n_estimators = 150\n",
    "\n",
    "def classify_gboost(X_train, X_test, y_train, y_test):        \n",
    "    clf = GradientBoostingClassifier(n_estimators = 150,\n",
    "                                     learning_rate = 0.5,\n",
    "                                     max_depth = 1, \n",
    "                                     random_state = 42)\n",
    "\n",
    "    clf.fit(X_train, y_train)\n",
    "    \n",
    "    print(\"[{}] Accuracy: train = {}, test = {}\".format(\n",
    "            clf.__class__.__name__,\n",
    "            clf.score(X_train, y_train),\n",
    "            clf.score(X_test, y_test)))\n",
    "    \n",
    "    return clf\n",
    "\n",
    "\n",
    "clf2 = classify_gboost(features_train_gradient,\n",
    "                       features_test_gradient,\n",
    "                       labels_train_gradient,\n",
    "                       labels_test_gradient)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5291e77f",
   "metadata": {},
   "source": [
    "## Deep Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0a1ec276",
   "metadata": {},
   "outputs": [],
   "source": [
    "def crear_model(mod, emb_size, vocabulary_size, max_words):\n",
    "    embedding_size = emb_size\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(vocabulary_size, embedding_size, input_length = max_words))\n",
    "    model.add(mod(100))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    print(model.summary())\n",
    "    return model\n",
    "\n",
    "def entreno(batch_size, num_epochs, X_train, labels_train, model):\n",
    "    \n",
    "    X_valid, y_valid = X_train[:batch_size], labels_train[:batch_size]  # first batch_size samples\n",
    "    X_train2, y_train2 = X_train[batch_size:], labels_train[batch_size:]  # rest for training\n",
    "\n",
    "    model.fit(X_train2, y_train2,\n",
    "              validation_data=(X_valid, y_valid),\n",
    "              batch_size=batch_size, epochs=num_epochs)\n",
    "    return model\n",
    "\n",
    "def evaluacion(model, X_test, labels_test):        \n",
    "    print(\"Test accuracy:\", model.evaluate(X_test, labels_test, verbose=0)[1]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a28db2aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Nuevo modelo\n",
      "\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 57, 32)            499840    \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 100)               53200     \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 553,141\n",
      "Trainable params: 553,141\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "\n",
      "Comienza entrenamiento\n",
      "\n",
      "Epoch 1/3\n",
      "78/78 [==============================] - 4s 38ms/step - loss: 0.6576 - accuracy: 0.6088 - val_loss: 0.5578 - val_accuracy: 0.7031\n",
      "Epoch 2/3\n",
      "78/78 [==============================] - 3s 34ms/step - loss: 0.4123 - accuracy: 0.8231 - val_loss: 0.4766 - val_accuracy: 0.7969\n",
      "Epoch 3/3\n",
      "78/78 [==============================] - 3s 32ms/step - loss: 0.2443 - accuracy: 0.9074 - val_loss: 0.5043 - val_accuracy: 0.8125\n",
      "\n",
      "Comienza evaluación\n",
      "\n",
      "Test accuracy: 0.7946345806121826\n",
      "\n",
      "Nuevo modelo\n",
      "\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 57, 32)            499840    \n",
      "_________________________________________________________________\n",
      "gru (GRU)                    (None, 100)               40200     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 540,141\n",
      "Trainable params: 540,141\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "\n",
      "Comienza entrenamiento\n",
      "\n",
      "Epoch 1/3\n",
      "78/78 [==============================] - 4s 34ms/step - loss: 0.6258 - accuracy: 0.6456 - val_loss: 0.5024 - val_accuracy: 0.7656\n",
      "Epoch 2/3\n",
      "78/78 [==============================] - 2s 30ms/step - loss: 0.3618 - accuracy: 0.8452 - val_loss: 0.4589 - val_accuracy: 0.8125\n",
      "Epoch 3/3\n",
      "78/78 [==============================] - 2s 30ms/step - loss: 0.2195 - accuracy: 0.9137 - val_loss: 0.5531 - val_accuracy: 0.8125\n",
      "\n",
      "Comienza evaluación\n",
      "\n",
      "Test accuracy: 0.7987974286079407\n",
      "\n",
      "Nuevo modelo\n",
      "\n",
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 57, 32)            499840    \n",
      "_________________________________________________________________\n",
      "simple_rnn (SimpleRNN)       (None, 100)               13300     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 513,241\n",
      "Trainable params: 513,241\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "\n",
      "Comienza entrenamiento\n",
      "\n",
      "Epoch 1/3\n",
      "78/78 [==============================] - 2s 18ms/step - loss: 0.6934 - accuracy: 0.5281 - val_loss: 0.6843 - val_accuracy: 0.6094\n",
      "Epoch 2/3\n",
      "78/78 [==============================] - 1s 16ms/step - loss: 0.5966 - accuracy: 0.7147 - val_loss: 0.6865 - val_accuracy: 0.6094\n",
      "Epoch 3/3\n",
      "78/78 [==============================] - 1s 15ms/step - loss: 0.3478 - accuracy: 0.8735 - val_loss: 0.9218 - val_accuracy: 0.5312\n",
      "\n",
      "Comienza evaluación\n",
      "\n",
      "Test accuracy: 0.5564292073249817\n"
     ]
    }
   ],
   "source": [
    "for mod in [LSTM, GRUV2, SimpleRNN]:\n",
    "    print(\"\\nNuevo modelo\\n\")\n",
    "    model = crear_model(mod, 32, len(vocabulary), maxlen)    \n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    print(\"\\nComienza entrenamiento\\n\")\n",
    "    entreno(64, 3, features_train, labels_train, model)\n",
    "    print(\"\\nComienza evaluación\\n\")\n",
    "    evaluacion(model, features_test, labels_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "411a8d79",
   "metadata": {},
   "source": [
    "## RNN + word2vec Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dcf7d2cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import multiprocessing as mp\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import (\n",
    "    Dense,\n",
    "    Dropout,\n",
    "    Embedding,\n",
    "    LSTM,\n",
    ")\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ca5a77d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pipeline_word2vec(X_train, X_test):\n",
    "    # WORD2VEC\n",
    "    W2V_SIZE = 300\n",
    "    W2V_WINDOW = 7\n",
    "    # 32\n",
    "    W2V_EPOCH = 5\n",
    "    W2V_MIN_COUNT = 1\n",
    "    # KERAS\n",
    "    SEQUENCE_LENGTH = 500\n",
    "    \n",
    "    def generate_tokenizer(train_df):\n",
    "        tokenizer = Tokenizer()\n",
    "        tokenizer.fit_on_texts(train_df)\n",
    "        vocab_size = len(tokenizer.word_index) + 1\n",
    "        print(f\"Total words: {vocab_size}\")\n",
    "        return tokenizer, vocab_size\n",
    "    \n",
    "    def generate_word2vec(train_df):\n",
    "        documents = [_text.split() for _text in train_df.review]\n",
    "        w2v_model = gensim.models.word2vec.Word2Vec(\n",
    "            vector_size=W2V_SIZE,\n",
    "            window=W2V_WINDOW,\n",
    "            min_count=W2V_MIN_COUNT,\n",
    "            workers=mp.cpu_count(),\n",
    "        )\n",
    "        w2v_model.build_vocab(documents)\n",
    "\n",
    "        words = list(w2v_model.wv.index_to_key)\n",
    "        vocab_size = len(words)\n",
    "        print(f\"Vocab size: {vocab_size}\")\n",
    "        w2v_model.train(documents, total_examples=len(documents), epochs=W2V_EPOCH)\n",
    "\n",
    "        return w2v_model\n",
    "    \n",
    "    def generate_embedding(word2vec_model, vocab_size, tokenizer):\n",
    "        embedding_matrix = np.zeros((vocab_size, W2V_SIZE))\n",
    "        for word, i in tokenizer.word_index.items():\n",
    "            if word in word2vec_model.wv:\n",
    "                embedding_matrix[i] = word2vec_model.wv[word]\n",
    "        return Embedding(\n",
    "            vocab_size,\n",
    "            W2V_SIZE,\n",
    "            weights=[embedding_matrix],\n",
    "            input_length=SEQUENCE_LENGTH,\n",
    "            trainable=False,\n",
    "        )\n",
    "    \n",
    "    X_train_words = pd.DataFrame(list(X_train), columns=[\"review\"])\n",
    "    X_test_words = pd.DataFrame(list(X_test), columns=[\"review\"])\n",
    "    \n",
    "    tokenizer, vocab = generate_tokenizer(X_train_words.review)\n",
    "    \n",
    "    word2vec_model = generate_word2vec(X_train_words)\n",
    "    \n",
    "    max_words = 500\n",
    "\n",
    "    X_train_words = sequence.pad_sequences(\n",
    "        tokenizer.texts_to_sequences(X_train_words.review), maxlen=max_words)\n",
    "    X_test_words = sequence.pad_sequences(\n",
    "        tokenizer.texts_to_sequences(X_test_words.review), maxlen=max_words)\n",
    "    \n",
    "    embedding_layer = generate_embedding(word2vec_model, vocab, tokenizer)\n",
    "   \n",
    "    return embedding_layer, X_train_words, X_test_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7996a54f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total words: 25649\n",
      "Vocab size: 56278\n"
     ]
    }
   ],
   "source": [
    "embedding_layer, X_train_words, X_test_words = pipeline_word2vec(X_train, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bd936cd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_custom = Sequential()\n",
    "model_custom.add(embedding_layer)\n",
    "model_custom.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))\n",
    "model_custom.add(Dense(1, activation=\"sigmoid\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d280fcbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_custom.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "8cdc96e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "78/78 [==============================] - 50s 642ms/step - loss: 0.6205 - accuracy: 0.6536 - val_loss: 0.5214 - val_accuracy: 0.7656\n",
      "Epoch 2/5\n",
      "78/78 [==============================] - 49s 625ms/step - loss: 0.5969 - accuracy: 0.6727 - val_loss: 0.5345 - val_accuracy: 0.7500\n",
      "Epoch 3/5\n",
      "78/78 [==============================] - 51s 649ms/step - loss: 0.5857 - accuracy: 0.6853 - val_loss: 0.4749 - val_accuracy: 0.8281\n",
      "Epoch 4/5\n",
      "78/78 [==============================] - 47s 601ms/step - loss: 0.5595 - accuracy: 0.7112 - val_loss: 0.5024 - val_accuracy: 0.7812\n",
      "Epoch 5/5\n",
      "78/78 [==============================] - 45s 574ms/step - loss: 0.5427 - accuracy: 0.7245 - val_loss: 0.4485 - val_accuracy: 0.8125\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f481a9d81c0>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 64\n",
    "num_epochs = 5\n",
    "\n",
    "X_train_words_valid, y_valid = X_train_words[:batch_size], labels_train[:batch_size]  # first batch_size samples\n",
    "X_train_words2, y_train2 = X_train_words[batch_size:], labels_train[batch_size:]  # rest for training\n",
    "\n",
    "model_custom.fit(X_train_words2, y_train2,\n",
    "          validation_data=(X_train_words_valid, y_valid),\n",
    "          batch_size=batch_size, epochs=num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "6d8902dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 0.7132284641265869\n"
     ]
    }
   ],
   "source": [
    "evaluacion(model_custom, X_test_words, labels_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02ec08da",
   "metadata": {},
   "source": [
    "# Parte 4 - Reporte de métricas y conclusiones"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7c6f305",
   "metadata": {},
   "source": [
    "### LSTM, GRUV2\n",
    "##### Vemos que en este caso el mejor modelo ha sido GRUV2 con:\n",
    " * val_accuracy: 0.8125\n",
    " * Test accuracy: 0.7987974286079407.\n",
    "\n",
    "#### Sin embargo no mejora el resultado obtenido con la regresion logística en el notebook anterios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "50062727",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c_params    10.000000\n",
      "train        0.952815\n",
      "test         0.806198\n",
      "Name: 8, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame(columns = ['c_params', 'train', 'test'])\n",
    "\n",
    "for c in [0.01, 0.02, 0.03, 0.04, 0.05, 0.25, 0.5, 1, 10, 100, 1000]:\n",
    "    \n",
    "    lr = LogisticRegression(C = c, solver = 'lbfgs', max_iter = 500)\n",
    "    lr.fit(features_train_gradient, labels_train_gradient)\n",
    "    \n",
    "    df = df.append({'c_params' : c,\n",
    "                    'train' : accuracy_score(labels_train_gradient, \n",
    "                                             lr.predict(features_train_gradient)),\n",
    "                    'test' : accuracy_score(labels_test_gradient,\n",
    "                                            lr.predict(features_test_gradient))},\n",
    "                   ignore_index = True)\n",
    "\n",
    "print(df.sort_values('test').iloc[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "6d2444c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confussion matrix:\n",
      "[[874 196]\n",
      " [223 869]]\n",
      "\n",
      "Classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.82      0.81      1070\n",
      "           1       0.82      0.80      0.81      1092\n",
      "\n",
      "    accuracy                           0.81      2162\n",
      "   macro avg       0.81      0.81      0.81      2162\n",
      "weighted avg       0.81      0.81      0.81      2162\n",
      "\n",
      "Accuracy score:0.8061979648473635\n"
     ]
    }
   ],
   "source": [
    "lr = LogisticRegression(C = 10, solver = 'lbfgs', max_iter = 500)\n",
    "lr.fit(features_train_gradient, labels_train_gradient)\n",
    "    \n",
    "print(f'Confussion matrix:\\n{confusion_matrix(labels_test_gradient, lr.predict(features_test_gradient))}')\n",
    "print(f'\\nClassification report:\\n{classification_report(labels_test_gradient, lr.predict(features_test_gradient))}')\n",
    "print(f'Accuracy score:{accuracy_score(labels_test_gradient, lr.predict(features_test_gradient))}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f229b7d",
   "metadata": {},
   "source": [
    "#### Se observa que tampoco mejora el resultado. Además vemos que existe overfitting\n",
    " * train        0.952815\n",
    " * test         0.806198"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
